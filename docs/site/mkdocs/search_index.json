{
    "docs": [
        {
            "location": "/", 
            "text": "ReverseDiff.jl\n\n\nReverseDiff implements methods to take \ngradients\n, \nJacobians\n, \nHessians\n, and higher-order derivatives of native Julia functions (or any callable object, really) using \nreverse mode automatic differentiation (AD)\n.\n\n\nWhile performance can vary depending on the functions you evaluate, the algorithms implemented by ReverseDiff \ngenerally outperform non-AD algorithms in both speed and accuracy.\n\n\nWikipedia's entry on automatic differentiation\n is a useful resource for learning about the advantages of AD techniques over other common differentiation methods (such as \nfinite differencing\n).\n\n\n\n\nInstallation\n\n\nTo install ReverseDiff, simply use Julia's package manager:\n\n\njulia\n Pkg.add(\nReverseDiff\n)\n\n\n\n\nThe current version of ReverseDiff supports Julia v0.5 and v0.6.\n\n\n\n\nWhy use ReverseDiff?\n\n\nOther Julia packages may provide some of these features, but only ReverseDiff provides all of them (as far as I know at the time of this writing):\n\n\n\n\nsupports most of the Julia language, including loops, recursion, and control flow\n\n\nuser-friendly API for reusing and compiling tapes\n\n\nuser-friendly performance annotations such as \n@forward\n and \n@skip\n (with more to come!)\n\n\ncompatible with ForwardDiff, enabling mixed-mode AD\n\n\nbuilt-in definitions leverage the benefits of ForwardDiff's \nDual\n numbers (e.g. SIMD, zero-overhead arithmetic)\n\n\na familiar differentiation API for ForwardDiff users\n\n\nnon-allocating linear algebra optimizations\n\n\nsuitable as an execution backend for graphical machine learning libraries\n\n\nReverseDiff doesn't need to record scalar indexing operations (a huge cost for many similar libraries)\n\n\nhigher-order \nmap\n and \nbroadcast\n optimizations\n\n\n\n\n\n\nShould I use ReverseDiff or ForwardDiff?\n\n\nForwardDiff is theoretically more efficient for differentiating functions where the input dimension is less than the output dimension, while ReverseDiff is theoretically more efficient for differentiating functions where the output dimension is less than the input dimension.\n\n\nThus, ReverseDiff is generally a better choice for gradients, but Jacobians and Hessians are trickier to determine. For example, optimized methods for computing nested derivatives might use a combination of forward-mode and reverse-mode AD.\n\n\nForwardDiff is often faster than ReverseDiff for lower dimensional gradients (\nlength(input) \n 100\n), or gradients of functions that are implemented as very large programs.\n\n\nIn general, your choice of algorithms will depend on the function being differentiated, and you should benchmark different methods to see how they fare.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#reversediffjl", 
            "text": "ReverseDiff implements methods to take  gradients ,  Jacobians ,  Hessians , and higher-order derivatives of native Julia functions (or any callable object, really) using  reverse mode automatic differentiation (AD) .  While performance can vary depending on the functions you evaluate, the algorithms implemented by ReverseDiff  generally outperform non-AD algorithms in both speed and accuracy.  Wikipedia's entry on automatic differentiation  is a useful resource for learning about the advantages of AD techniques over other common differentiation methods (such as  finite differencing ).", 
            "title": "ReverseDiff.jl"
        }, 
        {
            "location": "/#installation", 
            "text": "To install ReverseDiff, simply use Julia's package manager:  julia  Pkg.add( ReverseDiff )  The current version of ReverseDiff supports Julia v0.5 and v0.6.", 
            "title": "Installation"
        }, 
        {
            "location": "/#why-use-reversediff", 
            "text": "Other Julia packages may provide some of these features, but only ReverseDiff provides all of them (as far as I know at the time of this writing):   supports most of the Julia language, including loops, recursion, and control flow  user-friendly API for reusing and compiling tapes  user-friendly performance annotations such as  @forward  and  @skip  (with more to come!)  compatible with ForwardDiff, enabling mixed-mode AD  built-in definitions leverage the benefits of ForwardDiff's  Dual  numbers (e.g. SIMD, zero-overhead arithmetic)  a familiar differentiation API for ForwardDiff users  non-allocating linear algebra optimizations  suitable as an execution backend for graphical machine learning libraries  ReverseDiff doesn't need to record scalar indexing operations (a huge cost for many similar libraries)  higher-order  map  and  broadcast  optimizations", 
            "title": "Why use ReverseDiff?"
        }, 
        {
            "location": "/#should-i-use-reversediff-or-forwarddiff", 
            "text": "ForwardDiff is theoretically more efficient for differentiating functions where the input dimension is less than the output dimension, while ReverseDiff is theoretically more efficient for differentiating functions where the output dimension is less than the input dimension.  Thus, ReverseDiff is generally a better choice for gradients, but Jacobians and Hessians are trickier to determine. For example, optimized methods for computing nested derivatives might use a combination of forward-mode and reverse-mode AD.  ForwardDiff is often faster than ReverseDiff for lower dimensional gradients ( length(input)   100 ), or gradients of functions that are implemented as very large programs.  In general, your choice of algorithms will depend on the function being differentiated, and you should benchmark different methods to see how they fare.", 
            "title": "Should I use ReverseDiff or ForwardDiff?"
        }, 
        {
            "location": "/limits/", 
            "text": "Limitations of ReverseDiff\n\n\nReverseDiff works by injecting user code with new number types that record all operations that occur on them, accumulating an execution trace of the target function which can be re-run forwards and backwards to propagate new input values and derivative information. Naturally, this technique has some limitations. Here's a list of all the roadblocks we've seen users run into (\"target function\" here refers to the function being differentiated):\n\n\n\n\nThe target function can only be composed of generic Julia functions.\n ReverseDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of \nAx_mul_Bx\n-style functions.\n\n\nThe target function must be written generically enough to accept numbers of type \nT\n:Real\n as input (or arrays of these numbers).\n The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well.\n\n\nNested differentiation of closures is dangerous.\n Differentiating closures is safe, and nested differentation is safe, but you might be vulnerable to a subtle bug if you try to do both. See \nthis ForwardDiff issue\n for details. A fix is currently being planned for this problem.\n\n\nThe types of array inputs must be subtypes of \nAbstractArray\n.\n\n\nArray inputs that are being differentiated cannot be mutated\n. This also applies to any \"descendent\" arrays that must be tracked (e.g. if \nA\n is an immutable input array, then \nC = A * A\n will also be immutable). If you try to perform \nsetindex!\n on such arrays, an error will be thrown. In the future, this restriction might be lifted. Note that arrays explicitly constructed within the target function (e.g. via \nones\n, \nsimilar\n, etc.) can be mutated, as well as output arrays used when taking the Jacobian of a function of the form \nf!(output, input....).", 
            "title": "Limitations of ReverseDiff"
        }, 
        {
            "location": "/limits/#limitations-of-reversediff", 
            "text": "ReverseDiff works by injecting user code with new number types that record all operations that occur on them, accumulating an execution trace of the target function which can be re-run forwards and backwards to propagate new input values and derivative information. Naturally, this technique has some limitations. Here's a list of all the roadblocks we've seen users run into (\"target function\" here refers to the function being differentiated):   The target function can only be composed of generic Julia functions.  ReverseDiff cannot propagate derivative information through non-Julia code. Thus, your function may not work if it makes calls to external, non-Julia programs, e.g. uses explicit BLAS calls instead of  Ax_mul_Bx -style functions.  The target function must be written generically enough to accept numbers of type  T :Real  as input (or arrays of these numbers).  The function doesn't require a specific type signature, as long as the type signature is generic enough to avoid breaking this rule. This also means that any storage assigned used within the function must be generic as well.  Nested differentiation of closures is dangerous.  Differentiating closures is safe, and nested differentation is safe, but you might be vulnerable to a subtle bug if you try to do both. See  this ForwardDiff issue  for details. A fix is currently being planned for this problem.  The types of array inputs must be subtypes of  AbstractArray .  Array inputs that are being differentiated cannot be mutated . This also applies to any \"descendent\" arrays that must be tracked (e.g. if  A  is an immutable input array, then  C = A * A  will also be immutable). If you try to perform  setindex!  on such arrays, an error will be thrown. In the future, this restriction might be lifted. Note that arrays explicitly constructed within the target function (e.g. via  ones ,  similar , etc.) can be mutated, as well as output arrays used when taking the Jacobian of a function of the form  f!(output, input....).", 
            "title": "Limitations of ReverseDiff"
        }, 
        {
            "location": "/api/", 
            "text": "ReverseDiff API\n\n\n\n\nGradients of \nf(x::AbstractArray{Real}...)::Real\n\n\n#\n\n\nReverseDiff.gradient\n \n \nFunction\n.\n\n\nReverseDiff.gradient(f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nf\n has the form \nf(::AbstractArray{Real})::Real\n and return \n\u2207f(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nf\n has the form \nf(::AbstractArray{Real}...)::Real\n (such that it can be called as \nf(input...)\n) and return a \nTuple\n where the \ni\nth element is the gradient of \nf\n w.r.t. \ninput[i].\n\n\nNote that \ncfg\n can be preallocated and reused for subsequent calls.\n\n\nIf possible, it is highly recommended to use \nReverseDiff.GradientTape\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\n#\n\n\nReverseDiff.gradient!\n \n \nFunction\n.\n\n\nReverseDiff.gradient!(result, f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.gradient(f, input, cfg)\n, except it stores the resulting gradient(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\nReverseDiff.gradient!(tape::Union{GradientTape,CompiledGradient}, input)\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \ntape\n represents a function of the form \nf(::AbstractArray)::Real\n and return \n\u2207f(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \ntape\n represents a function of the form \nf(::AbstractArray...)::Real\n and return a \nTuple\n where the \ni\nth element is the gradient of \nf\n w.r.t. \ninput[i].\n\n\nsource\n\n\nReverseDiff.gradient!(result, tape::Union{GradientTape,CompiledGradient}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.gradient!(tape, input)\n, except it stores the resulting gradient(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\n\n\nJacobians of \nf(x::AbstractArray{Real}...)::AbstractArray{Real}\n\n\n#\n\n\nReverseDiff.jacobian\n \n \nFunction\n.\n\n\nReverseDiff.jacobian(f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \nf\n has the form \nf(::AbstractArray{Real})::AbstractArray{Real}\n and return \nJ(f)(input)\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \nf\n has the form \nf(::AbstractArray{Real}...)::AbstractArray{Real}\n (such that it can be called as \nf(input...)\n) and return a \nTuple\n where the \ni\nth element is the  Jacobian of \nf\n w.r.t. \ninput[i].\n\n\nNote that \ncfg\n can be preallocated and reused for subsequent calls.\n\n\nIf possible, it is highly recommended to use \nReverseDiff.JacobianTape\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\nReverseDiff.jacobian(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nExactly like \nReverseDiff.jacobian(f, input, cfg)\n, except the target function has the form \nf!(output::AbstractArray{Real}, input::AbstractArray{Real}...)\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.jacobian!\n \n \nFunction\n.\n\n\nReverseDiff.jacobian!(result, f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.jacobian(f, input, cfg)\n, except it stores the resulting Jacobian(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value \nf(input)\n (or \nf(input...)\n, if \nisa(input, Tuple)\n) will be stored in it as well.\n\n\nsource\n\n\nReverseDiff.jacobian!(result, f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nExactly like \nReverseDiff.jacobian!(result, f, input, cfg)\n, except the target function has the form \nf!(output::AbstractArray{Real}, input::AbstractArray{Real}...)\n.\n\n\nsource\n\n\nReverseDiff.jacobian!(tape::Union{JacobianTape,CompiledJacobian}, input)\n\n\n\n\nIf \ninput\n is an \nAbstractArray\n, assume \ntape\n represents a function of the form \nf(::AbstractArray{Real})::AbstractArray{Real}\n or \nf!(::AbstractArray{Real}, ::AbstractArray{Real})\n and return \ntape\n's Jacobian w.r.t. \ninput\n.\n\n\nIf \ninput\n is a tuple of \nAbstractArray\ns, assume \ntape\n represents a function of the form \nf(::AbstractArray{Real}...)::AbstractArray{Real}\n or \nf!(::AbstractArray{Real}, ::AbstractArray{Real}...)\n and return a \nTuple\n where the \ni\nth element is \ntape\n's Jacobian w.r.t. \ninput[i].\n\n\nNote that if \ntape\n represents a function of the form \nf!(output, input...)\n, you can only execute \ntape\n with new \ninput\n values. There is no way to re-run \ntape\n's tape with new \noutput\n values; since \nf!\n can mutate \noutput\n, there exists no stable \"hook\" for loading new \noutput\n values into the tape.\n\n\nsource\n\n\nReverseDiff.jacobian!(result, tape::Union{JacobianTape,CompiledJacobian}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.jacobian!(tape, input)\n, except it stores the resulting Jacobian(s) in \nresult\n rather than allocating new memory.\n\n\nresult\n can be an \nAbstractArray\n or a \nTuple\n of \nAbstractArray\ns. The \nresult\n (or any of its elements, if \nisa(result, Tuple)\n), can also be a \nDiffBase.DiffResult\n, in which case the primal value of the target function will be stored in it as well.\n\n\nsource\n\n\n\n\nHessians of \nf(x::AbstractArray{Real})::Real\n\n\n#\n\n\nReverseDiff.hessian\n \n \nFunction\n.\n\n\nReverseDiff.hessian(f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\n\n\n\nGiven \nf(input::AbstractArray{Real})::Real\n, return \nf\ns Hessian w.r.t. to the given \ninput\n.\n\n\nNote that \ncfg\n can be preallocated and reused for subsequent calls.\n\n\nIf possible, it is highly recommended to use \nReverseDiff.HessianTape\n to prerecord \nf\n. Otherwise, this method will have to re-record \nf\n's execution trace for every subsequent call.\n\n\nsource\n\n\n#\n\n\nReverseDiff.hessian!\n \n \nFunction\n.\n\n\nReverseDiff.hessian!(result::AbstractArray, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\nReverseDiff.hessian!(result::DiffResult, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(result, input))\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.hessian(f, input, cfg)\n, except it stores the resulting Hessian in \nresult\n rather than allocating new memory.\n\n\nIf \nresult\n is a \nDiffBase.DiffResult\n, the primal value \nf(input)\n and the gradient \n\u2207f(input)\n will be stored in it along with the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\nReverseDiff.hessian!(tape::Union{HessianTape,CompiledHessian}, input)\n\n\n\n\nAssuming \ntape\n represents a function of the form \nf(::AbstractArray{Real})::Real\n, return the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\nReverseDiff.hessian!(result::AbstractArray, tape::Union{HessianTape,CompiledHessian}, input)\n\nReverseDiff.hessian!(result::DiffResult, tape::Union{HessianTape,CompiledHessian}, input)\n\n\n\n\nReturns \nresult\n. This method is exactly like \nReverseDiff.hessian!(tape, input)\n, except it stores the resulting Hessian in \nresult\n rather than allocating new memory.\n\n\nIf \nresult\n is a \nDiffBase.DiffResult\n, the primal value \nf(input)\n and the gradient \n\u2207f(input)\n will be stored in it along with the Hessian \nH(f)(input)\n.\n\n\nsource\n\n\n\n\nThe \nAbstractTape\n API\n\n\nReverseDiff works by recording the target function's execution trace to a \"tape\", then running the tape forwards and backwards to propagate new input values and derivative information.\n\n\nIn many cases, it is the recording phase of this process that consumes the most time and memory, while the forward and reverse execution passes are often fast and non-allocating. Luckily, ReverseDiff provides the \nAbstractTape\n family of types, which enable the user to \npre-record\n a reusable tape for a given function and differentiation operation.\n\n\nNote that pre-recording a tape can only capture the the execution trace of the target function with the given input values.\n Therefore, re-running the tape (even with new input values) will only execute the paths that were recorded using the original input values. In other words, the tape cannot any re-enact branching behavior that depends on the input values. You can guarantee your own safety in this regard by never using the \nAbstractTape\n API with functions that contain control flow based on the input values.\n\n\nSimilarly to the branching issue, a tape is not guaranteed to capture any side-effects caused or depended on by the target function.\n\n\n#\n\n\nReverseDiff.GradientTape\n \n \nType\n.\n\n\nReverseDiff.GradientTape(f, input, cfg::GradientConfig = GradientConfig(input))\n\n\n\n\nReturn a \nGradientTape\n instance containing a pre-recorded execution trace of \nf\n at the given \ninput\n.\n\n\nThis \nGradientTape\n can then be passed to \nReverseDiff.gradient!\n to take gradients of the execution trace with new \ninput\n values.\n\n\nSee \nReverseDiff.gradient\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.JacobianTape\n \n \nType\n.\n\n\nReverseDiff.JacobianTape(f, input, cfg::JacobianConfig = JacobianConfig(input))\n\n\n\n\nReturn a \nJacobianTape\n instance containing a pre-recorded execution trace of \nf\n at the given \ninput\n.\n\n\nThis \nJacobianTape\n can then be passed to \nReverseDiff.jacobian!\n to take Jacobians of the execution trace with new \ninput\n values.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\nReverseDiff.JacobianTape(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))\n\n\n\n\nReturn a \nJacobianTape\n instance containing a pre-recorded execution trace of \nf\n at the given \noutput\n and \ninput\n.\n\n\nThis \nJacobianTape\n can then be passed to \nReverseDiff.jacobian!\n to take Jacobians of the execution trace with new \ninput\n values.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.HessianTape\n \n \nType\n.\n\n\nReverseDiff.HessianTape(f, input, cfg::HessianConfig = HessianConfig(input))\n\n\n\n\nReturn a \nHessianTape\n instance containing a pre-recorded execution trace of \nf\n at the given \ninput\n.\n\n\nThis \nHessianTape\n can then be passed to \nReverseDiff.hessian!\n to take Hessians of the execution trace with new \ninput\n values.\n\n\nSee \nReverseDiff.hessian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.compile\n \n \nFunction\n.\n\n\nReverseDiff.compile(t::AbstractTape)\n\n\n\n\nReturn a fully compiled representation of \nt\n. The type of this representation will be \nCompiledGradient\n/\nCompiledJacobian\n/\nCompiledHessian\n, depending on the type of \nt\n. This object can be passed to any API methods that accept \nt\n.\n\n\nIn many cases, compiling \nt\n can significantly speed up execution time. Note that the longer the tape, the more time compilation may take. Very long tapes (i.e. when \nlength(t)\n is on the order of 10000 elements) can take a very long time to compile.\n\n\nsource\n\n\n\n\nThe \nAbstractConfig\n API\n\n\nFor the sake of convenience and performance, all \"extra\" information used by ReverseDiff's API methods is bundled up in the \nReverseDiff.AbstractConfig\n family of types. These types allow the user to easily feed several different parameters to ReverseDiff's API methods, such as work buffers and tape configurations.\n\n\nReverseDiff's basic API methods will allocate these types automatically by default, but you can reduce memory usage and improve performance if you preallocate them yourself.\n\n\n#\n\n\nReverseDiff.GradientConfig\n \n \nType\n.\n\n\nReverseDiff.GradientConfig(input, tp::RawTape = RawTape())\n\n\n\n\nReturn a \nGradientConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.gradient\n/\nReverseDiff.gradient!\n methods.\n\n\nNote that \ninput\n is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of \ninput\n is same as the element type of the target function's output.\n\n\nSee \nReverseDiff.gradient\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\nReverseDiff.GradientConfig(input, ::Type{D}, tp::RawTape = RawTape())\n\n\n\n\nLike \nGradientConfig(input, tp)\n, except the provided type \nD\n is assumed to be the element type of the target function's output.\n\n\nsource\n\n\n#\n\n\nReverseDiff.JacobianConfig\n \n \nType\n.\n\n\nReverseDiff.JacobianConfig(input, tp::RawTape = RawTape())\n\n\n\n\nReturn a \nJacobianConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.jacobian\n/\nReverseDiff.jacobian!\n methods.\n\n\nNote that \ninput\n is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of \ninput\n is same as the element type of the target function's output.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nReverseDiff.JacobianConfig(input, ::Type{D}, tp::RawTape = RawTape())\n\n\n\n\nLike \nJacobianConfig(input, tp)\n, except the provided type \nD\n is assumed to be the element type of the target function's output.\n\n\nsource\n\n\nReverseDiff.JacobianConfig(output::AbstractArray, input, tp::RawTape = RawTape())\n\n\n\n\nReturn a \nJacobianConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.jacobian\n/\nReverseDiff.jacobian!\n methods. This method assumes the target function has the form \nf!(output, input)\n\n\nNote that \ninput\n and \noutput\n are only used for type and shape information; they are not stored or modified in any way.\n\n\nSee \nReverseDiff.jacobian\n for a description of acceptable types for \ninput\n.\n\n\nsource\n\n\nReverseDiff.JacobianConfig(result::DiffBase.DiffResult, input, tp::RawTape = RawTape())\n\n\n\n\nA convenience method for \nJacobianConfig(DiffBase.value(result), input, tp)\n.\n\n\nsource\n\n\n#\n\n\nReverseDiff.HessianConfig\n \n \nType\n.\n\n\nReverseDiff.HessianConfig(input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())\n\n\n\n\nReturn a \nHessianConfig\n instance containing the preallocated tape and work buffers used by the \nReverseDiff.hessian\n/\nReverseDiff.hessian!\n methods. \ngtp\n is the tape used for the inner gradient calculation, while \njtp\n is used for outer Jacobian calculation.\n\n\nNote that \ninput\n is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of \ninput\n is same as the element type of the target function's output.\n\n\nsource\n\n\nReverseDiff.HessianConfig(input::AbstractArray, ::Type{D}, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())\n\n\n\n\nLike \nHessianConfig(input, tp)\n, except the provided type \nD\n is assumed to be the element type of the target function's output.\n\n\nsource\n\n\nReverseDiff.HessianConfig(result::DiffBase.DiffResult, input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())\n\n\n\n\nLike \nHessianConfig(input, tp)\n, but utilize \nresult\n along with \ninput\n to construct work buffers.\n\n\nNote that \nresult\n and \ninput\n are only used for type and shape information; they are not stored or modified in any way.\n\n\nsource", 
            "title": "ReverseDiff API"
        }, 
        {
            "location": "/api/#reversediff-api", 
            "text": "", 
            "title": "ReverseDiff API"
        }, 
        {
            "location": "/api/#gradients-of-fxabstractarrayrealreal", 
            "text": "#  ReverseDiff.gradient     Function .  ReverseDiff.gradient(f, input, cfg::GradientConfig = GradientConfig(input))  If  input  is an  AbstractArray , assume  f  has the form  f(::AbstractArray{Real})::Real  and return  \u2207f(input) .  If  input  is a tuple of  AbstractArray s, assume  f  has the form  f(::AbstractArray{Real}...)::Real  (such that it can be called as  f(input...) ) and return a  Tuple  where the  i th element is the gradient of  f  w.r.t.  input[i].  Note that  cfg  can be preallocated and reused for subsequent calls.  If possible, it is highly recommended to use  ReverseDiff.GradientTape  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  #  ReverseDiff.gradient!     Function .  ReverseDiff.gradient!(result, f, input, cfg::GradientConfig = GradientConfig(input))  Returns  result . This method is exactly like  ReverseDiff.gradient(f, input, cfg) , except it stores the resulting gradient(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source  ReverseDiff.gradient!(tape::Union{GradientTape,CompiledGradient}, input)  If  input  is an  AbstractArray , assume  tape  represents a function of the form  f(::AbstractArray)::Real  and return  \u2207f(input) .  If  input  is a tuple of  AbstractArray s, assume  tape  represents a function of the form  f(::AbstractArray...)::Real  and return a  Tuple  where the  i th element is the gradient of  f  w.r.t.  input[i].  source  ReverseDiff.gradient!(result, tape::Union{GradientTape,CompiledGradient}, input)  Returns  result . This method is exactly like  ReverseDiff.gradient!(tape, input) , except it stores the resulting gradient(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source", 
            "title": "Gradients of f(x::AbstractArray{Real}...)::Real"
        }, 
        {
            "location": "/api/#jacobians-of-fxabstractarrayrealabstractarrayreal", 
            "text": "#  ReverseDiff.jacobian     Function .  ReverseDiff.jacobian(f, input, cfg::JacobianConfig = JacobianConfig(input))  If  input  is an  AbstractArray , assume  f  has the form  f(::AbstractArray{Real})::AbstractArray{Real}  and return  J(f)(input) .  If  input  is a tuple of  AbstractArray s, assume  f  has the form  f(::AbstractArray{Real}...)::AbstractArray{Real}  (such that it can be called as  f(input...) ) and return a  Tuple  where the  i th element is the  Jacobian of  f  w.r.t.  input[i].  Note that  cfg  can be preallocated and reused for subsequent calls.  If possible, it is highly recommended to use  ReverseDiff.JacobianTape  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  ReverseDiff.jacobian(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Exactly like  ReverseDiff.jacobian(f, input, cfg) , except the target function has the form  f!(output::AbstractArray{Real}, input::AbstractArray{Real}...) .  source  #  ReverseDiff.jacobian!     Function .  ReverseDiff.jacobian!(result, f, input, cfg::JacobianConfig = JacobianConfig(input))  Returns  result . This method is exactly like  ReverseDiff.jacobian(f, input, cfg) , except it stores the resulting Jacobian(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value  f(input)  (or  f(input...) , if  isa(input, Tuple) ) will be stored in it as well.  source  ReverseDiff.jacobian!(result, f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Exactly like  ReverseDiff.jacobian!(result, f, input, cfg) , except the target function has the form  f!(output::AbstractArray{Real}, input::AbstractArray{Real}...) .  source  ReverseDiff.jacobian!(tape::Union{JacobianTape,CompiledJacobian}, input)  If  input  is an  AbstractArray , assume  tape  represents a function of the form  f(::AbstractArray{Real})::AbstractArray{Real}  or  f!(::AbstractArray{Real}, ::AbstractArray{Real})  and return  tape 's Jacobian w.r.t.  input .  If  input  is a tuple of  AbstractArray s, assume  tape  represents a function of the form  f(::AbstractArray{Real}...)::AbstractArray{Real}  or  f!(::AbstractArray{Real}, ::AbstractArray{Real}...)  and return a  Tuple  where the  i th element is  tape 's Jacobian w.r.t.  input[i].  Note that if  tape  represents a function of the form  f!(output, input...) , you can only execute  tape  with new  input  values. There is no way to re-run  tape 's tape with new  output  values; since  f!  can mutate  output , there exists no stable \"hook\" for loading new  output  values into the tape.  source  ReverseDiff.jacobian!(result, tape::Union{JacobianTape,CompiledJacobian}, input)  Returns  result . This method is exactly like  ReverseDiff.jacobian!(tape, input) , except it stores the resulting Jacobian(s) in  result  rather than allocating new memory.  result  can be an  AbstractArray  or a  Tuple  of  AbstractArray s. The  result  (or any of its elements, if  isa(result, Tuple) ), can also be a  DiffBase.DiffResult , in which case the primal value of the target function will be stored in it as well.  source", 
            "title": "Jacobians of f(x::AbstractArray{Real}...)::AbstractArray{Real}"
        }, 
        {
            "location": "/api/#hessians-of-fxabstractarrayrealreal", 
            "text": "#  ReverseDiff.hessian     Function .  ReverseDiff.hessian(f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))  Given  f(input::AbstractArray{Real})::Real , return  f s Hessian w.r.t. to the given  input .  Note that  cfg  can be preallocated and reused for subsequent calls.  If possible, it is highly recommended to use  ReverseDiff.HessianTape  to prerecord  f . Otherwise, this method will have to re-record  f 's execution trace for every subsequent call.  source  #  ReverseDiff.hessian!     Function .  ReverseDiff.hessian!(result::AbstractArray, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(input))\n\nReverseDiff.hessian!(result::DiffResult, f, input::AbstractArray, cfg::HessianConfig = HessianConfig(result, input))  Returns  result . This method is exactly like  ReverseDiff.hessian(f, input, cfg) , except it stores the resulting Hessian in  result  rather than allocating new memory.  If  result  is a  DiffBase.DiffResult , the primal value  f(input)  and the gradient  \u2207f(input)  will be stored in it along with the Hessian  H(f)(input) .  source  ReverseDiff.hessian!(tape::Union{HessianTape,CompiledHessian}, input)  Assuming  tape  represents a function of the form  f(::AbstractArray{Real})::Real , return the Hessian  H(f)(input) .  source  ReverseDiff.hessian!(result::AbstractArray, tape::Union{HessianTape,CompiledHessian}, input)\n\nReverseDiff.hessian!(result::DiffResult, tape::Union{HessianTape,CompiledHessian}, input)  Returns  result . This method is exactly like  ReverseDiff.hessian!(tape, input) , except it stores the resulting Hessian in  result  rather than allocating new memory.  If  result  is a  DiffBase.DiffResult , the primal value  f(input)  and the gradient  \u2207f(input)  will be stored in it along with the Hessian  H(f)(input) .  source", 
            "title": "Hessians of f(x::AbstractArray{Real})::Real"
        }, 
        {
            "location": "/api/#the-abstracttape-api", 
            "text": "ReverseDiff works by recording the target function's execution trace to a \"tape\", then running the tape forwards and backwards to propagate new input values and derivative information.  In many cases, it is the recording phase of this process that consumes the most time and memory, while the forward and reverse execution passes are often fast and non-allocating. Luckily, ReverseDiff provides the  AbstractTape  family of types, which enable the user to  pre-record  a reusable tape for a given function and differentiation operation.  Note that pre-recording a tape can only capture the the execution trace of the target function with the given input values.  Therefore, re-running the tape (even with new input values) will only execute the paths that were recorded using the original input values. In other words, the tape cannot any re-enact branching behavior that depends on the input values. You can guarantee your own safety in this regard by never using the  AbstractTape  API with functions that contain control flow based on the input values.  Similarly to the branching issue, a tape is not guaranteed to capture any side-effects caused or depended on by the target function.  #  ReverseDiff.GradientTape     Type .  ReverseDiff.GradientTape(f, input, cfg::GradientConfig = GradientConfig(input))  Return a  GradientTape  instance containing a pre-recorded execution trace of  f  at the given  input .  This  GradientTape  can then be passed to  ReverseDiff.gradient!  to take gradients of the execution trace with new  input  values.  See  ReverseDiff.gradient  for a description of acceptable types for  input .  source  #  ReverseDiff.JacobianTape     Type .  ReverseDiff.JacobianTape(f, input, cfg::JacobianConfig = JacobianConfig(input))  Return a  JacobianTape  instance containing a pre-recorded execution trace of  f  at the given  input .  This  JacobianTape  can then be passed to  ReverseDiff.jacobian!  to take Jacobians of the execution trace with new  input  values.  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  source  ReverseDiff.JacobianTape(f!, output, input, cfg::JacobianConfig = JacobianConfig(output, input))  Return a  JacobianTape  instance containing a pre-recorded execution trace of  f  at the given  output  and  input .  This  JacobianTape  can then be passed to  ReverseDiff.jacobian!  to take Jacobians of the execution trace with new  input  values.  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  source  #  ReverseDiff.HessianTape     Type .  ReverseDiff.HessianTape(f, input, cfg::HessianConfig = HessianConfig(input))  Return a  HessianTape  instance containing a pre-recorded execution trace of  f  at the given  input .  This  HessianTape  can then be passed to  ReverseDiff.hessian!  to take Hessians of the execution trace with new  input  values.  See  ReverseDiff.hessian  for a description of acceptable types for  input .  source  #  ReverseDiff.compile     Function .  ReverseDiff.compile(t::AbstractTape)  Return a fully compiled representation of  t . The type of this representation will be  CompiledGradient / CompiledJacobian / CompiledHessian , depending on the type of  t . This object can be passed to any API methods that accept  t .  In many cases, compiling  t  can significantly speed up execution time. Note that the longer the tape, the more time compilation may take. Very long tapes (i.e. when  length(t)  is on the order of 10000 elements) can take a very long time to compile.  source", 
            "title": "The AbstractTape API"
        }, 
        {
            "location": "/api/#the-abstractconfig-api", 
            "text": "For the sake of convenience and performance, all \"extra\" information used by ReverseDiff's API methods is bundled up in the  ReverseDiff.AbstractConfig  family of types. These types allow the user to easily feed several different parameters to ReverseDiff's API methods, such as work buffers and tape configurations.  ReverseDiff's basic API methods will allocate these types automatically by default, but you can reduce memory usage and improve performance if you preallocate them yourself.  #  ReverseDiff.GradientConfig     Type .  ReverseDiff.GradientConfig(input, tp::RawTape = RawTape())  Return a  GradientConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.gradient / ReverseDiff.gradient!  methods.  Note that  input  is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of  input  is same as the element type of the target function's output.  See  ReverseDiff.gradient  for a description of acceptable types for  input .  source  ReverseDiff.GradientConfig(input, ::Type{D}, tp::RawTape = RawTape())  Like  GradientConfig(input, tp) , except the provided type  D  is assumed to be the element type of the target function's output.  source  #  ReverseDiff.JacobianConfig     Type .  ReverseDiff.JacobianConfig(input, tp::RawTape = RawTape())  Return a  JacobianConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.jacobian / ReverseDiff.jacobian!  methods.  Note that  input  is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of  input  is same as the element type of the target function's output.  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  ReverseDiff.JacobianConfig(input, ::Type{D}, tp::RawTape = RawTape())  Like  JacobianConfig(input, tp) , except the provided type  D  is assumed to be the element type of the target function's output.  source  ReverseDiff.JacobianConfig(output::AbstractArray, input, tp::RawTape = RawTape())  Return a  JacobianConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.jacobian / ReverseDiff.jacobian!  methods. This method assumes the target function has the form  f!(output, input)  Note that  input  and  output  are only used for type and shape information; they are not stored or modified in any way.  See  ReverseDiff.jacobian  for a description of acceptable types for  input .  source  ReverseDiff.JacobianConfig(result::DiffBase.DiffResult, input, tp::RawTape = RawTape())  A convenience method for  JacobianConfig(DiffBase.value(result), input, tp) .  source  #  ReverseDiff.HessianConfig     Type .  ReverseDiff.HessianConfig(input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())  Return a  HessianConfig  instance containing the preallocated tape and work buffers used by the  ReverseDiff.hessian / ReverseDiff.hessian!  methods.  gtp  is the tape used for the inner gradient calculation, while  jtp  is used for outer Jacobian calculation.  Note that  input  is only used for type and shape information; it is not stored or modified in any way. It is assumed that the element type of  input  is same as the element type of the target function's output.  source  ReverseDiff.HessianConfig(input::AbstractArray, ::Type{D}, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())  Like  HessianConfig(input, tp) , except the provided type  D  is assumed to be the element type of the target function's output.  source  ReverseDiff.HessianConfig(result::DiffBase.DiffResult, input::AbstractArray, gtp::RawTape = RawTape(), jtp::RawTape = RawTape())  Like  HessianConfig(input, tp) , but utilize  result  along with  input  to construct work buffers.  Note that  result  and  input  are only used for type and shape information; they are not stored or modified in any way.  source", 
            "title": "The AbstractConfig API"
        }
    ]
}